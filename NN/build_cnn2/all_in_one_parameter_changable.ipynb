{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["RkvdZFDJK7_7"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Initial Part"],"metadata":{"id":"iR7gJLdQNedl"}},{"cell_type":"markdown","source":["## Mount google-drive & Load dataset"],"metadata":{"id":"TiNr_gp1Hm1J"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#shared path to drive\n","SHARED_PATH=\"/content/drive/MyDrive/Classroom/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours\"\n","\n","#direct path to drive\n","DIRECT_PATH=\"/content/drive/MyDrive/Classroom/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMtWKi8SI_U7","executionInfo":{"status":"ok","timestamp":1750254480140,"user_tz":-330,"elapsed":1993,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}},"outputId":"3143c551-78f7-4dee-b548-09be829c3833"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Determine the data path based on the existence of shared or direct paths\n","root_path = SHARED_PATH if os.path.exists(SHARED_PATH) else DIRECT_PATH if os.path.exists(DIRECT_PATH) else None\n","\n","DATA_PATH = ''\n","# Print the chosen path or an error message\n","if root_path:\n","    print(f\"Using path: {root_path}\")\n","    DATA_PATH = root_path + \"/Proj2_Heritage_places_in_CGR_classify\"\n","    for filename in os.listdir(DATA_PATH):\n","        print(filename)\n","else:\n","    print(\"Neither shared nor direct path exists. Please check the paths.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlRbiljJPAgi","executionInfo":{"status":"ok","timestamp":1750254480143,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}},"outputId":"30727147-abb3-4d20-b256-e10c36818338"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Using path: /content/drive/MyDrive/Classroom/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours\n","Data\n","old_experiments\n","expr2_effnet.ipynb\n","expr1_resnet18.ipynb\n","training_images\n","validate_images\n","data.csv\n","maximize_dataset_make_csv_data.ipynb\n"]}]},{"cell_type":"markdown","source":["##  Setup Device & Random State"],"metadata":{"id":"Y2pWhdELT8CR"}},{"cell_type":"code","source":["import torch\n","import random\n","import numpy as np\n","\n","# Define device (CPU or GPU)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)\n","\n","# define the random seed\n","SEED = 2025\n","\n","torch.manual_seed(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","os.environ['PYTHONHASHSEED'] = str(SEED)"],"metadata":{"id":"vnXDdqOGg7oH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750254480144,"user_tz":-330,"elapsed":9,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}},"outputId":"3fd4a9ec-1989-44eb-f350-3740ed66ce5c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["## Define data transformations"],"metadata":{"id":"frsQDDsBT0On"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","\n","mean=[0.485, 0.456, 0.406]\n","standev=[0.229, 0.224, 0.225]\n","px=256\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((px, px)),\n","    transforms.RandomCrop((224, 224)),\n","    transforms.RandomRotation(15, fill=10), # Increased rotation degrees\n","    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n","    # transforms.RandomVerticalFlip(), # Uncomment if vertical flips are appropriate\n","    # transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(mean), std=(standev)),\n","])\n","\n","test_transform =  transforms.Compose([\n","    transforms.Resize((px, px)),\n","    transforms.CenterCrop((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(mean), std=(standev))\n","])\n","\n","print(train_transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgGo5q0PS18n","executionInfo":{"status":"ok","timestamp":1750254480171,"user_tz":-330,"elapsed":27,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}},"outputId":"b63293e6-8773-4782-c566-7f339265212c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Compose(\n","    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","    RandomCrop(size=(224, 224), padding=None)\n","    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=10)\n","    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), scale=(0.9, 1.1))\n","    ToTensor()\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",")\n"]}]},{"cell_type":"markdown","source":["## Define data path"],"metadata":{"id":"CdaxupWwNQOV"}},{"cell_type":"code","source":["train_path = DATA_PATH + '/training_images'\n","test_path = DATA_PATH + '/validate_images'"],"metadata":{"id":"lxxaIspaNVid","executionInfo":{"status":"ok","timestamp":1750254480188,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Load dataset with Torch"],"metadata":{"id":"cPAAYEi8WIkO"}},{"cell_type":"code","source":["from torchvision.datasets import ImageFolder\n","\n","train_dataset = ImageFolder(root=DATA_PATH+'/training_images', transform=train_transform)\n","test_dataset = ImageFolder(root=DATA_PATH+'/validate_images', transform=test_transform)\n","\n","CLASSES  = train_dataset.classes\n","print(CLASSES)\n","\n","print(train_dataset)\n","print(test_dataset)"],"metadata":{"id":"e73fIp7jFy9d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"675101f8-a477-47c6-8dd5-d4b7e690a07d","executionInfo":{"status":"ok","timestamp":1750254480204,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["['charch', 'clock_tower', 'jora_ghat', 'mondir', 'musium', 'patalbari']\n","Dataset ImageFolder\n","    Number of datapoints: 309\n","    Root location: /content/drive/MyDrive/Classroom/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours/Proj2_Heritage_places_in_CGR_classify/training_images\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","               RandomCrop(size=(224, 224), padding=None)\n","               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=10)\n","               RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), scale=(0.9, 1.1))\n","               ToTensor()\n","               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","           )\n","Dataset ImageFolder\n","    Number of datapoints: 15\n","    Root location: /content/drive/MyDrive/Classroom/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours/ChandernagoreCollegeSemVI-Addon 2025 Sem VI Computer Science Honours/Proj2_Heritage_places_in_CGR_classify/validate_images\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n","               CenterCrop(size=(224, 224))\n","               ToTensor()\n","               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","           )\n"]}]},{"cell_type":"code","source":["print(train_dataset.class_to_idx)\n","print(len(train_dataset.class_to_idx))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbvr3Rf16EvB","outputId":"0c4b4791-f0eb-4264-caee-453a1c2816b2","executionInfo":{"status":"ok","timestamp":1750254480280,"user_tz":-330,"elapsed":75,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["{'charch': 0, 'clock_tower': 1, 'jora_ghat': 2, 'mondir': 3, 'musium': 4, 'patalbari': 5}\n","6\n"]}]},{"cell_type":"markdown","source":["## Class definition"],"metadata":{"id":"RkvdZFDJK7_7"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"2RT56m8WP7t2","executionInfo":{"status":"ok","timestamp":1750254480285,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from tqdm import tqdm\n","\n","# Define the BuildingClassifier class\n","class BuildingClassifier:\n","    \"\"\"\n","    A class to classify buildings and their facades using a given model.\n","    \"\"\"\n","    # Initialize the classifier with model, loss functions, optimizer, and data parameters\n","    def __init__(self, model, optimizer, batch_size, loss_fn=nn.CrossEntropyLoss(),\n","    train_dataset=train_dataset, test_dataset=test_dataset,\n","    categories=train_dataset.classes, device=DEVICE):\n","        \"\"\"\n","        Initializes the BuildingClassifier.\n","        Args:\n","            model (torch.nn.Module): The neural network model to use for classification.\n","            building_loss_fn (torch.nn.Module): The loss function for building classification.\n","            facade_loss_fn (torch.nn.Module): The loss function for facade classification.\n","            optimizer (torch.optim.Optimizer): The optimizer for model training.\n","            learning_rate (float): The learning rate for the optimizer.\n","            batch_size (int): The batch size for data loaders.\n","            train_dataset (torch.utils.data.Dataset): The training dataset.\n","            test_dataset (torch.utils.data.Dataset): The testing dataset.\n","            categories (list): A list of building categories.\n","            device (torch.device): The device (CPU/GPU) to train the model on.\n","        \"\"\"\n","        self.model = model.to(device)  # Move model to the specified device (CPU/GPU)\n","        self.loss_fn = loss_fn # Loss function for building classification\n","        self.optimizer = optimizer # Optimizer for model training\n","        self.batch_size = batch_size # Batch size for data loaders\n","        self.train_dataset = train_dataset # Training dataset\n","        self.test_dataset = test_dataset # Testing dataset\n","        self.categories = categories # List of building categories\n","        self.device = device # Device to train the model on\n","        self.learning_rate = self.optimizer.param_groups[0]['lr']# Learning rate for the optimizer\n","\n","        # Create DataLoaders for training and testing\n","        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n","        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n","\n","\n","    ############################################################################\n","    # Train the model for a specified number of epochs\n","    def train(self, n_epochs=10, output_logging=True):\n","        loss_graph_list, accy_list, valid_loss_list = [], [], [] # Lists to store training metrics\n","        print(f\"Starting training with Learning Rate: {self.learning_rate}, Batch Size: {self.batch_size}, Epochs: {n_epochs}, Device: {self.device}\")\n","\n","        for epoch in tqdm(range(n_epochs)):\n","            self.model.train()  # Set the model to training mode\n","            training_loss = [] # List to store loss for the current epoch\n","\n","            # Iterate over the training data\n","            for i_image, i_label in self.train_loader:\n","                i_image = i_image.to(self.device) # Move image to device\n","                i_label = i_label.to(self.device) # Move label to device\n","\n","                self.optimizer.zero_grad() # Zero the gradients\n","                output = self.model(i_image) # Get model outputs for building and facade\n","\n","                loss = self.loss_fn(output, i_label) # Calculate building loss\n","                loss.backward(retain_graph=True) # Backpropagate building loss, retain graph for facade loss\n","\n","                training_loss.append(loss.item()) # Append loss to list\n","\n","            if output_logging:\n","                print(f\"Training loss at iteration--{epoch+1} is:- {np.mean(training_loss)}\")\n","            loss_graph_list.append(np.mean(training_loss)) # Store average training loss\n","\n","            accy, current_valid_loss = self.evaluate(output_logging=output_logging) # Eval on the valid set\n","            valid_loss_list.append(current_valid_loss) # Store validation loss\n","            accy_list.append(accy) # Store accuracy\n","\n","        print(\"Training done.....................\")\n","        return loss_graph_list, valid_loss_list, accy_list # Return training metrics\n","\n","\n","    ############################################################################\n","    # Evaluate the model on the test set\n","    def evaluate(self, output_logging=True, disp_confusion_matrix=False):\n","        self.model.eval() # Set the model to evaluation mode\n","        pred_correctly = 0 # Counter for correctly predicted samples\n","        total = 0 # Counter for total samples\n","        actual_labels = [] # List to store actual labels\n","        predicted_labels = [] # List to store predicted labels\n","        validation_loss = [] # List to store validation loss\n","\n","        with torch.no_grad(): # Disable gradient calculation\n","            for i_image, i_label in self.test_loader:\n","                i_image = i_image.to(self.device) # Move image to device\n","                i_label = i_label.to(self.device) # Move label to device\n","\n","                total += i_label.size(0) # Increment total samples\n","                output = self.model(i_image) # Get model outputs\n","\n","                loss = self.loss_fn(output, i_label) # Calculate building loss\n","\n","                _, predicted = torch.max(output.data, 1) # Get predicted class for building\n","                pred_correctly += (predicted == i_label).sum().item() # Count correct predictions\n","\n","                actual_labels.extend(i_label.cpu().numpy()) # Store actual labels\n","                predicted_labels.extend(predicted.cpu().numpy()) # Store predicted labels\n","\n","        accuracy = 100.0 * pred_correctly / total # Calculate accuracy\n","        average_validation_loss = np.mean(validation_loss) # Calculate average validation loss\n","\n","        if output_logging:\n","            print(f' --Validation loss:- {average_validation_loss}', end='')\n","            print(f' --Testing....got-({pred_correctly}/{total})-correctly-->>accuracy=({accuracy:.2f}%)')\n","\n","        # Display confusion matrix if requested\n","        if disp_confusion_matrix:\n","            display_labels_list = [str(label) for label in self.categories]\n","            ConfusionMatrixDisplay(\n","                confusion_matrix=confusion_matrix(actual_labels, predicted_labels),\n","                display_labels=display_labels_list\n","            ).plot()\n","            plt.xticks(rotation=90)\n","            plt.show()\n","\n","        return accuracy, average_validation_loss # Return accuracy and validation loss\n","\n","\n","    ############################################################################\n","    # Classify a single image\n","    def classify_image(self, img_path, img_transform):\n","        \"\"\"\n","        Classifies a single image.\n","        Args:\n","            img_path (str): The path to the image.\n","            img_transform (callable): The image transform to apply.\n","        \"\"\"\n","        self.model.eval() # Set the model to evaluation mode\n","        input_img = Image.open(DATA_PATH + img_path) # Load the image\n","\n","        plt.figure(figsize=(10, 10),dpi=30) # Create a figure for plotting\n","        plt.imshow(input_img) # Display the image\n","        plt.axis('off') # Turn off the axis\n","        plt.show() # Show the plot\n","\n","        img_tensor = img_transform(input_img).unsqueeze(0).to(self.device) # Transform and prepare the image tensor\n","        with torch.no_grad(): # Disable gradient calculation\n","            output = self.model(img_tensor) # Get model output\n","            probabilities1 = torch.nn.functional.softmax(output[0], dim=1) # Calculate probabilities for building classes\n","            print(probabilities1)\n","            _, predicted_class_index = torch.max(probabilities1, 1) # Get predicted building class index\n","            predicted_class = self.categories[predicted_class_index.item()] # Get predicted building class name\n","            print(f\"Predicted class: {predicted_class}\") # Print predictions"]},{"cell_type":"markdown","metadata":{"id":"d7405be4"},"source":["## Plotting Training Results"]},{"cell_type":"code","metadata":{"id":"28e98627","executionInfo":{"status":"ok","timestamp":1750254480305,"user_tz":-330,"elapsed":18,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"source":["import matplotlib.pyplot as plt\n","\n","def ploting(ax, arr, color,lname, ylabel='loss', xlabel='epoch'):\n","    ax.plot(arr, color=color, label=lname)\n","    ax.scatter(range(len(arr)), arr, color=color)\n","    ax.set_ylabel(ylabel, color=color)\n","    ax.tick_params(axis='y', labelcolor=color)\n","    ax.set_xlabel(xlabel)\n","\n","\n","def plot_classifier_results(train_loss, valid_loss, accy, classifier_name):\n","    fig, ax1 = plt.subplots(figsize=(10, 4), dpi=90)\n","    ax2 = ax1.twinx()\n","\n","    ploting(ax1, train_loss, color='red', lname=f'Training Loss ({classifier_name})')\n","    ploting(ax1, valid_loss, color='blue', lname=f'Validation Loss ({classifier_name})')\n","    ploting(ax2, accy, color='green', lname=f'Test Accuracy (%) ({classifier_name})', ylabel='Accuracy (%)')\n","\n","    handles1, labels1 = ax1.get_legend_handles_labels()\n","    handles2, labels2 = ax2.get_legend_handles_labels()\n","    plt.legend(handles1 + handles2, labels1 + labels2, loc='lower left')\n","\n","    plt.title(f'Training Loss, Validation Loss, and Test Accuracy per Epoch ({classifier_name})')\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Modify the last layer\n","\n","\n","\n"],"metadata":{"id":"HV1pt9OORpVM"}},{"source":["def modify_last_layer(model, layer_type, pretrained, categories=train_dataset.classes):\n","    model._is_pretrained_backbone = pretrained\n","    # Assuming the last layer is named 'fc' as in many torchvision models\n","    last_layer_in = model.fc.in_features\n","    no_of_classes = len(categories)\n","    new_mid_layer_out = 256 # You can adjust this based on your needs\n","\n","    if layer_type == 'linear':\n","        print(f'convert from {last_layer_in} to-> {no_of_classes} output')\n","        model.fc = nn.Linear(last_layer_in, no_of_classes)\n","    elif layer_type == 'sequential':\n","        print(f'convert from {last_layer_in} to-> {new_mid_layer_out} to-> {no_of_classes} output')\n","        model.fc = nn.Sequential(\n","        torch.nn.Linear(in_features=last_layer_in, out_features=new_mid_layer_out),\n","        torch.nn.ReLU(),\n","        torch.nn.Dropout(p=0.2),\n","        torch.nn.Linear(in_features=new_mid_layer_out,\n","                        out_features=no_of_classes,),\n","        # torch.nn.Softmax(dim=1), handels by crossentropyloss()\n","    )\n","\n","    if pretrained:\n","        model = fc_weight_bias_init(model)\n","    else:\n","        model = all_weight_bias_init(model)\n","    return model"],"cell_type":"code","metadata":{"id":"CsdoohCWLUtC","executionInfo":{"status":"ok","timestamp":1750254480327,"user_tz":-330,"elapsed":20,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["import torch.nn.init as init\n","def fc_weight_bias_init(model):\n","    for module in model.fc.modules():\n","        if isinstance(module, nn.Linear):\n","            init.kaiming_normal_(\n","                module.weight, a=0, mode='fan_out', nonlinearity='relu',\n","            )\n","            if module.bias is not None:\n","                fan_in, fan_out = init._calculate_fan_in_and_fan_out(module.weight)\n","                bound = 1 / (fan_out)**0.5\n","                init.normal_(module.bias, -bound, bound)\n","    return model\n","\n","def all_weight_bias_init(model):\n","    for module in model.modules():\n","        if isinstance(module, nn.Linear):\n","            init.kaiming_normal_(\n","                module.weight, a=0, mode='fan_out', nonlinearity='relu',\n","            )\n","            if module.bias is not None:\n","                fan_in, fan_out = init._calculate_fan_in_and_fan_out(module.weight)\n","                bound = 1 / (fan_out)**0.5\n","                init.normal_(module.bias, -bound, bound)\n","    return model"],"metadata":{"id":"UeHD-EimOf1B","executionInfo":{"status":"ok","timestamp":1750254480391,"user_tz":-330,"elapsed":75,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Resets the weights of the entire model instance using the custom init_weights function.\n","def reset_model(model):\n","    if model._is_pretrained_backbone:\n","        model.apply(fc_weight_bias_init)\n","        print('reset last')\n","    else:\n","        model.apply(all_weight_bias_init)\n","        print('reset all')"],"metadata":{"id":"6ZN4DVd4Rjo5","executionInfo":{"status":"ok","timestamp":1750254480395,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shanku Bag","userId":"18150558297276386800"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# Main part"],"metadata":{"id":"ms1Gqmf9Nk_U"}},{"cell_type":"markdown","source":["## Create instances and calsify"],"metadata":{"id":"RwSdgKb8NvgL"}},{"cell_type":"markdown","source":["### Model 1 ResNet18 non pretrained"],"metadata":{"id":"Yo6ltXLUYTWg"}},{"cell_type":"code","source":["from sys import last_type\n","# test model 1\n","import torchvision.models as models\n","model1 = models.resnet18(weights=None)\n","model1 = modify_last_layer(model1, layer_type='linear', pretrained=False)\n"],"metadata":{"id":"MNfTNsHIRJcs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Classifier 1"],"metadata":{"id":"3LntBa3GRPO8"}},{"cell_type":"code","source":["classifier1 = BuildingClassifier(\n","    model=model1,\n","    optimizer=optim.Adam(model1.parameters(), lr=0.001),\n","    batch_size=16,\n",")\n","\n","# change output_logging=True to print output of each iteration\n","train_loss1, valid_loss1, accy1 = classifier1.train(n_epochs=15, output_logging=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zuMjQgmpQ1p2","outputId":"1f4c1704-b6ac-4ab8-fc6f-31654788e2ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training with Learning Rate: 0.001, Batch Size: 16, Epochs: 15, Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 7/15 [01:24<01:37, 12.19s/it]"]}]},{"cell_type":"code","source":["# plot results\n","plot_classifier_results(train_loss1, valid_loss1, accy1, 'Classifier 1 (ResNet18)')\n","classifier1.evaluate(disp_confusion_matrix=True)"],"metadata":{"id":"FA7TF9_USGzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Classifier 2"],"metadata":{"id":"mTHkkKzXRSvr"}},{"cell_type":"code","source":["# Instance 2 with different parameters\n","reset_model(model1) # NEEDED for clearing pre run data\n","classifier2 = BuildingClassifier(\n","    model=model1,\n","    optimizer=optim.Adam(model1.parameters(), lr=0.01),\n","    batch_size=32,\n",")\n","\n","# change output_logging=True to print output of each iteration\n","train_loss2, valid_loss2, accy2 = classifier2.train(n_epochs=10, output_logging=False)"],"metadata":{"id":"dT5M2GaMMrhU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot results\n","plot_classifier_results(train_loss2, valid_loss2, accy2, 'Classifier 2 (ResNet18)')\n","classifier2.evaluate(disp_confusion_matrix=True)"],"metadata":{"id":"LhktXhzWSc1q"},"execution_count":null,"outputs":[]}]}